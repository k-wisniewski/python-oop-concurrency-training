{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab85ecea-646d-4fdd-92c2-5d68c8495488",
   "metadata": {},
   "source": [
    "## Współbieżność i zrównoleglenie\n",
    "Często spotykamy się z sytuacją, gdy w mowie potocznej oba pojęcia używane są wymiennie. Z punktu widzenia informatyki, istnieje fundamentalna różnica między oboma pojęciami: współbieżność jest własnością kodu, natomiast zrównoleglenie dotyczy sposobu konkretnego wykonania. Można zatem myśleć o współbieżności jako o istnieniu \"okazji\" do równoległego wykonania kodu, z której w danej sytuacji można, ale nie trzeba korzystać. Implementacja programu może być więc współbieżna, ale na jednordzeniowym procesorze wykonanie siłą rzeczy równoległe nie będzie - program jednak będzie działał całkowicie poprawnie dzięki systemowi dzielenia czasu.\n",
    "\n",
    "W Pythonie istnieje kilka mechanizmów, które można użyć do stworzenia kodu współbieżnego:\n",
    "- wątki\n",
    "- procesy\n",
    "- generatory, korutyny i programowanie asynchroniczne\n",
    "\n",
    "Poniżej omówimy po kolei różne podejścia do pisania programów współbieżnych w Pythonie.\n",
    "\n",
    "### Wątki\n",
    "Jak wiele innych języków również w Pythonie dostepny jest mechanizm wątków, dzielących wspólną przestrzeń adresową, wspieranych przez system operacyjny (na uniksach z użyciem `pthreads`). Aby stworzyć nowe wątki możemy posłużyć się modułem `threading`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3de0b8-3180-4a49-b88a-900b49224925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread with id #123145474007040 - starting...\n",
      "thread with id #123145490796544 - starting...\n",
      "thread with id #123145507586048 - starting...\n",
      "thread with id #123145524375552 - starting...\n",
      "thread with id #123145541165056 - starting...\n",
      "thread: #123145474007040 - finished!thread: #123145490796544 - finished!\n",
      "\n",
      "thread: #123145524375552 - finished!\n",
      "thread: #123145507586048 - finished!\n",
      "main thread: joined thread #123145474007040\n",
      "main thread: joined thread #123145490796544\n",
      "main thread: joined thread #123145507586048\n",
      "main thread: joined thread #123145524375552\n",
      "thread: #123145541165056 - finished!\n",
      "main thread: joined thread #123145541165056\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, get_ident\n",
    "import time\n",
    "\n",
    "def foo():\n",
    "    print(f\"thread with id #{get_ident()} - starting...\")\n",
    "    time.sleep(5)\n",
    "    print(f\"thread: #{get_ident()} - finished!\")\n",
    "\n",
    "threads = []\n",
    "for _ in range(5):\n",
    "    t = Thread(target=foo)\n",
    "    t. start()\n",
    "    threads.append(t)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "    print(f\"main thread: joined thread #{t.ident}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d8a53-fcf3-4de5-abab-8b33fef3e704",
   "metadata": {},
   "source": [
    "Tak użyte wątki są jednak problematyczne - łatwo zapomnieć o wywołaniu `join()`, co prowadzi do wycieków pamięci. Również oczekiwanie jest nieintuicyjne - nie wiadomo, który wątek skończy się pierwszy, więc nie wiadomo na który czekać w pierszej kolejności. Na szczęście istnieje łatwiejsze rozwiązanie - moduł standardowej biblioteki `concurrent.futures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14766ac-92e8-4d93-9627-d80d135059f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread #0 with id #123145474007040 - starting...  total active: 9thread #1 with id #123145490796544 - starting...  total active: 10\n",
      "\n",
      "thread #2 with id #123145507586048 - starting...  total active: 11\n",
      "thread #2 with id #123145507586048 - finished!    total active: 11\n",
      "thread #3 with id #123145507586048 - starting...  total active: 11\n",
      "thread #1 with id #123145490796544 - finished!    total active: 11\n",
      "thread #4 with id #123145490796544 - starting...  total active: 11\n",
      "thread #0 with id #123145474007040 - finished!    total active: 11\n",
      "thread #4 with id #123145490796544 - finished!    total active: 10\n",
      "thread #3 with id #123145507586048 - finished!    total active: 9\n",
      "finished all - results are in order of increasing threadno: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from threading import get_ident, active_count\n",
    "import time\n",
    "\n",
    "\n",
    "def foo(threadNo):\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - starting...  total active: {active_count()}\")\n",
    "    time.sleep(5 - threadNo)\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - finished!    total active: {active_count()}\")\n",
    "    return threadNo\n",
    "    \n",
    "results = None\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = executor.map(foo, range(5))\n",
    "print(f\"finished all - results are in order of increasing threadno: {list(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed1dfc-ad17-45a6-b98f-45daa3552013",
   "metadata": {},
   "source": [
    "`active_count` daje tu zawyżone liczby bo dodatkowo Jupyter ma aktywne wątki w tym samym interpreterze. Użycie `ThreadPoolExecutor`a jako managera kontekstu jest teraz zalecanym sposobem tworzenia i zarządzania wątkami, który jest stosunkowo odporny na błędy programistyczne powodujące wycieki zasobów. Są jednak pułapki - przykładowo, jeśli do `.map` podamy funkcję bezargumentową, zostanie rzucony wyjątek, który jednak nie wydostanie się poza `ThreadPoolExecutor`, powodując trudne do debugowania bugi.\n",
    "Alternatywnie, możemy użyć innych metod executora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ebc49e-6722-4bf0-bbfe-d6bd159b4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "def foo(threadNo):\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - starting...  total active: {threading.active_count()}\")\n",
    "    time.sleep(5 - threadNo)\n",
    "    print(f\"thread #{threadNo} with id #{get_ident()} - finished!    total active: {threading.active_count()}\")\n",
    "    return threadNo\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(foo, threadNo) for threadNo in range(5)]\n",
    "    for f in concurrent.futures.as_completed(futures):\n",
    "        print(f\"main: finished {f.result()}\")\n",
    "print(\"finished all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbb689-c3a3-401b-8752-8c47849ced7b",
   "metadata": {},
   "source": [
    "`max_workers` specyfikuje ilość wątków, które będą wykonywać zadania stworzone przy użyciu`executor.submit`. Argumenty `submit` to funkcja implementująca kod zadania i argumenty, które mają być do niej przekazane.\n",
    "\n",
    "### Great Interpreter Lock (GIL) \n",
    "W odróżnieniu od większości innych nowoczesnych języków (poza np. Ruby), Python jest dość ograniczony jeśli chodzi o wykorzystanie wątków - na raz wykonuje się tylko jeden z nich, niezależnie od ilości dostępnych rdzeni. Wynika to głównie z gwarancji koniecznych do zapewnienia poprawnego działania garbage collectora - bez muteksa chroniącego liczniki referencji, niemożliwe byłoby zapewnienie bezpieczeństwa zwalniania zasobów. Ograniczenie to dotyczy jednak tylko \"kanonicznej\" implementacji Pythona - CPython, nie istnieje natomiast w PyPy czy IronPythonie. Ponieważ jednak zdecydowana większość programów używa CPythona, ograniczenie to sprawiło, że używanie wątków w rozumieniu biblioteki `threading` czy klasy `ThreadPoolExecutor` zalecane jest tylko dla problemów które są ograniczane przez IO tzw. `IO bound`. W przypadku gdy bottleneckiem jest CPU lepszym rozwiązaniem jest stworzenie osobnych procesów i standardowa komunikacja między nimi w oparciu o gniazda (*sockets*), pamięć dzieloną czy rurki (*pipes*). Wiąże się to jednak ze sporym narzutem, w związku z czym [PEP-703](https://peps.python.org/pep-0703/) postuluje przebudowę CPythona tak, by GIL stał się opcjonalny. Jeśli propozycja zostanie oficjalnie przyjęta - a po raz pierwszy od lat jest na to szansa, wątki staną się pełnoprawnym sposobem implementacji dowolnych programów współbieżnych. Póki co, wątki wykonują się pojedynczo, w związku z czym część programów daje poprawne wyniki, nawet jeśli nie jest do końca poprawna w klasycznym rozumieniu współbieżnośći:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c044f-54b7-4457-8174-495bf7e1356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "def bar(threadNo) -> int:\n",
    "    start = time.monotonic()\n",
    "    bla = 0\n",
    "    for _ in range(10_000_000):\n",
    "        bla += 1\n",
    "    print(f\"{threadNo} took {time.monotonic() - start}\")\n",
    "    return bla\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(bar, threadNo) for threadNo in range(10)]\n",
    "    for future in as_completed(futures):\n",
    "        print(f\"got {future.result()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefc4d1-d4d6-485a-9552-ce667c4c5d7f",
   "metadata": {},
   "source": [
    "### Wpływ GIL na problemy CPU-bound i na problemy IO-bound\n",
    "### Strategie radzenia sobie z ograniczeniami powodowanymi przez GIL\n",
    "\n",
    "#### Ogółem:\n",
    "1. Procesy zamiast wątków:\n",
    "Używaj wielu procesów zamiast wątków. Moduł multiprocessing w standardowej bibliotece Pythona pozwala na tworzenie procesów, które działają niezależnie i mogą być równocześnie wykonywane na wielordzeniowych procesorach, omijając GIL.\n",
    "2. Użyj innej implementacji Pythona:\n",
    "Nie wszystkie implementacje Pythona mają GIL. Na przykład, Jython (Python na JVM) i IronPython (Python na .NET) nie mają GIL. Alternatywą jest też PyPy, choć ma on własną wersję GIL, jest znacznie bardziej wydajny w wielu scenariuszach.\n",
    "3. Rozszerzenia w języku C/C++:\n",
    "Jeśli masz krytyczny dla wydajności fragment kodu, możesz napisać go jako rozszerzenie w C lub C++. W trakcie wykonywania kodu napisanego w C, GIL może być zwolniony.\n",
    "4. Asynchroniczność:\n",
    "Korzystaj z asynchronicznych bibliotek i frameworków, takich jak asyncio. Choć nadal podlegają one GIL, umożliwiają efektywne wykorzystanie jednego wątku poprzez asynchroniczne I/O i współprogramy.\n",
    "5. Zewnętrzne usługi:\n",
    "Jeśli ograniczenia GIL stają się problematyczne dla konkretnego zadania (np. obliczeń równoległych), można rozważyć offloading tych zadań do zewnętrznych usług lub systemów, które nie mają tych ograniczeń.\n",
    "6. Użyj wątków tylko dla operacji I/O:\n",
    "W wielu przypadkach GIL nie jest problemem, gdy wątki są używane głównie do operacji I/O (np. odczytu i zapisu do plików, komunikacji sieciowej), ponieważ GIL jest często zwalniany podczas takich operacji.\n",
    "\n",
    "#### Zwalnianie GIL w zewnętrznych bibliotekach\n",
    "Zewnętrzne biblioteki mogą używać dowolnej ilości wątków w sposób przezroczysty dla Pythona, pod warunkiem, że wykonają po swojej stronie całą synchronizację i interfejs z Pythonem będzie jednowatkowy. Jest to znaczące ułatwienie, choć nie całkowite remedium. Przykładowo, karty graficzne mają obecnie tak wielką wydajność i przepustowość pamięci, że samo odbieranie danych z powrotem do procesu Pythonowego jest sporym ograniczeniem. W wielu przypadkach jest to rozwiązanie, którego wpływ łatwo zmierzyć, toteż jest on relatywnie popularny. Przykładowo `NumPy` zawsze stara się pod spodem korzystać z bibliotek zaimplementowanych w C takich jak `OpenBLAS`czy `LAPACK`, które nie stronią od wykorzystania `OpenMP` i wielu wątków. Można to łatwo sprawdzić:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4896ff-6437-4e3b-8463-c6dc54f880f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8ccff-401d-447b-bd9b-4f68a6a1cac0",
   "metadata": {},
   "source": [
    "Idąc dalej można nawet zmierzyć wpływ ilości wątków skonfigurowanych w OpenMP, choćby na przykładzie mnożenia macierzy. Wykonajmy nastepujący program zmieniając wartość zmiennej środowiskowej `OMP_NUM_THREADS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3436b-7f54-487b-b498-bcf3723654c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "m = rng.random(size=(7000, 7000))\n",
    "m @ m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ed7a5-45cf-4d34-9891-9f57cf986f17",
   "metadata": {},
   "source": [
    "```\n",
    "$ export OMP_NUM_THREADS=1\n",
    "$ time python test.py\n",
    "\n",
    "real    0m13.782s\n",
    "user    0m13.702s\n",
    "sys     0m0.078s\n",
    "$ export OMP_NUM_THREADS=4\n",
    "$ time python test.py\n",
    "\n",
    "real    0m4.542s\n",
    "user    0m17.016s\n",
    "sys     0m0.558s\n",
    "```\n",
    "Niestety, nie każda biblioteka została zaimplementowana w C/C++ i my bindingi dla Pythona, choć ekosystem jest niezwykle bogaty.\n",
    "\n",
    "#### Własne rozszerzenie w C\n",
    "```\n",
    "#include <Python.h> // nagłówek z typami, funkcjami i makrami Pythona (dostępny zaraz po instalacji interpretera)\n",
    "\n",
    "int fibonacci(int n) { // ta funkcja nie może wywoływać API Pythona!!!\n",
    "    if (n == 0 || n == 1) {\n",
    "        return 1\n",
    "    } else {\n",
    "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
    "    }\n",
    "}\n",
    "\n",
    "static PyObject* wrapper(PyObject* self, PyObject* args) { // łączenie między Pythonem a C\n",
    "    int n, result;\n",
    "\n",
    "    if (!PyArg_ParseTuple(args, \"i\", &n)) {\n",
    "        return NULL;\n",
    "    }\n",
    "\n",
    "    Py_BEGIN_ALLOW_THREADS // Pozwalamy interpreterowi robić co innego kiedy fibonacci się wykonuje\n",
    "    result = fibonacci(n);\n",
    "    Py_END_ALLOW_THREADS // .. a tu wracamy\n",
    "\n",
    "    return Py_BuildValue(\"i\", result);\n",
    "}\n",
    "\n",
    "static PyMethodDef fibonacci_c_methods[] = {\n",
    "    {\"fibonacci\", wrrapper, METH_VARARGS, \"Liczy nta liczbe Fibonacciego\"},\n",
    "    {NULL, NULL, 0, NULL}\n",
    "};\n",
    "\n",
    "static struct PyModuleDef fibonacci_c_module = { // definicja modułu\n",
    "    PyModuleDef_HEAD_INIT,\n",
    "    \"fibonacci_c\", // tą nazwe importujemy\n",
    "    \"Moje rozszerzenie do liczenia fibonacciego\",\n",
    "    -1, // nie wspieramy subinterpreterów (PEP-554)\n",
    "    fibonacci_c_methods\n",
    "};\n",
    "\n",
    "PyMODINIT_FUNC PyInit_fibonacci_cmodule(void) { // odpalane podczas ładowania modułu\n",
    "    return PyModule_Create(&fibonacci_c_module);\n",
    "}\n",
    "```\n",
    "\n",
    "Możemy to teraz skompilować (zakładając, że nagłówki Pythona są w `/usr/include/Python3.11`):\n",
    "```\n",
    "$ gcc -I/usr/include/python3.11 -shared -fPIC -O3 -o fibonacci_c.so fibonacci_c.c\n",
    "```\n",
    "Po kompilacji możemy normalnie zaimportować nasz moduł przez `import fibonacci_c` i wołać funkcję przez `fibonacci_c.fibonacci`. W praktyce jest to dość żmudny proces i gdy chcemy uzyskać przenośny kod na wiele platform, musimy się sporo napracować. Warto wówczas zautomatyzować taki proces budowania i dystrybuować gotowe biblioteki jako `wheel`s (np. przez PyPI)\n",
    "\n",
    "#### Użycie NOGILa\n",
    "Po zaakceptowaniu PEP-703, począwszy od Pythona 3.13 powinien byc dostępny specjalny build CPythona, zbudowany z flagą `--disable-gil`. Jest to pierwsze podejście, które ma szansę na faktyczny sukces. W planach jest by docelowo CPython był oferowany wyłącznie bez GIL-a, ale przez kilka wydań będzie można korzystać z `GIL`-owej wersji i to ona na razie będzie domyślną. W tym czasie autorzy pythonowych bibliotek i rozszerzeń w C/C++ muszą dostosować swoje biblioteki do pracy z kodem, którego poprawnosc zależała od istnienia GILa. Do dopracowania pozostaje np. kwestia atomowości operacji na kolekcjach, jednak i tu pojawiają się sensowne rozwiązania. Zainteresowanych na razie trzeba odesłać do [tego repozytorium](https://github.com/colesbury/nogil) - jeśli Wasz kod zadziała na tej wersji to będzie działał już bez GIL-a. Można wtedy zmierzyć czy wydajność wzrosła i o ile, co pozwoli zdiagnozować rzeczywiste problemy i zoptymalizować implemeentację."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba720b-2b74-434f-825e-e1b7d8e7317a",
   "metadata": {},
   "source": [
    "### Interfejs concurrent.Futures.Executor\n",
    "`concurrent.futures` definiuje interfejs `Executor`, wyglądający mniej więcej tak:\n",
    "```\n",
    "class Executor:\n",
    "    def submit(fn, *args, **kwargs):\n",
    "        ...\n",
    "\n",
    "    def map(func, *iterables, timeout=None, chunksize=1):\n",
    "        ...\n",
    "\n",
    "    def shutdown(wait=True, *, cancel_futures=False):\n",
    "        ...\n",
    "```\n",
    "\n",
    "`submit` - zleca wykonanie na którymś z wątków-workerów funkcji `fn` z przekazanymi argumentami. Zwraca obiekt `Future`\n",
    "`map` - uruchamia funkcję `func` na każdym elemencie  kolekcji przekazanych w `iterables`. `chunksize` reguluje jak duże segmenty `iterables` zostają za jednym razem przekazane do workerów - ma to znaczenie zwłaszcza dla implementacji opartej o procesy, gdzie komunikacja międzyprocesowa może być relatywnie dość wolna. Gdy podamy argument `timeout`, podczas iterowania z użyciem zwracanego tu iteratora po upływie tego czasu `next()` spowoduje rzucenie wyjątku `TimeoutError`\n",
    "`shutdown` - sygnalizuje do executora, że należy zwolnić zasoby przez niego zaalokowane. Parametry regulują co dzieje się z niewykonanymi jeszcze obiektami `Future`.\n",
    "\n",
    "Executory implementowane przez bibliotekę `concurrent.futures` są też `contextmanager`ami - nie musimy wołać na nich `shutdown`, bo automatycznie stanie się to jeśli stworzymy executor w bloku `with`:\n",
    "\n",
    "```\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    ...\n",
    "# tu już jest po zawołaniu shutdown\n",
    "```\n",
    "\n",
    "Razem z biblioteką dostajemy 2 implementacje: znany nam już `ThreadPoolExecutor`, który zlecane zadania wykonuje na `max_workers` wątkach, oraz `ProcessPoolExecutor`, osiągający to samo przy użyciu procesów. Sprawia to, że stosunkowo łatwo jest wymienić niewłaściwie dobraną implementację - jeśli po sprofilowaniu okaże się np.: że ograniczeniem w naszym programie jest IO, możemy się pokusić o użycie `ThreadPoolExecutora`, który ma niższy narzut na stworzenie workerów i jest elastyczniejszy w komunikacji między workerami. Dla odmiany procesy ograniczane przez CPU będą znacząco lepiej wykorzystywały dostepne rdzenie procesora.\n",
    "\n",
    "### Procesy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63300e25-5760-4eb1-aa35-07d0977862f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "from multiprocessing import freeze_support\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    start = time.monotonic()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        print(f\"{name} took {time.monotonic() - start}\")\n",
    "\n",
    "def bar(threadNo) -> int:\n",
    "    with timer(threadNo):\n",
    "        bla = 0\n",
    "        for _ in range(10_000_000):\n",
    "            bla += 1\n",
    "        return bla\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(bar, threadNo) for threadNo in range(10)]\n",
    "        for future in as_completed(futures):\n",
    "            print(f\"got {future.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cec15b-3ad5-4338-9207-b45da68c41b9",
   "metadata": {},
   "source": [
    "Oczywiście, analogicznie do wątków, istnieje również [niskopoziomowe API](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing) do zarządzania procesami - służy do tego moduł `multiprocessing`, którego API jest bardzo podobne do `threading`. W wielu przypadkach jest ono jednak trudniejsze do użycia niż `concurrent.futures`, dlatego jeśli można, zaleca się by używać nowszego modułu `concurrent.futures`.\n",
    "\n",
    "### Wymiana danych między procesami\n",
    "Najprostsze w użyciu są 2 udostępniane przez bibliotekę multiprocessing sposoby wymiany danych między procesami: `Queue` i `Pipe`. `Queue` są thread- i process safe, pipe nie.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c0fba-2103-44a9-90f2-6cc92039dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def f(q):\n",
    "    q.put([42, None, 'hello']) # <- wysyłamy dane\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = Queue() # jedna współdzielona kolejka - muszę ją podać do funkcji implementującej ciało drugiego procesu\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    print(q.get()) # <- odbieramy dane\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16855c59-144a-41b5-a2d9-3459a2885bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def f(conn):\n",
    "    conn.send([42, None, 'hello']) # <- wysyłamy dane\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parent_conn, child_conn = Pipe() # dwie końcówki rurki - dla każdego procesu po 1\n",
    "    p = Process(target=f, args=(child_conn,)) # przekazanie drugiego końca rurki\n",
    "    p.start()\n",
    "    print(parent_conn.recv()) # <- odbieramy dane\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823b570-bba1-4531-8bf5-182552d8ee16",
   "metadata": {},
   "source": [
    "Można również używać pamięci dzielonej, która jest reprezentowana przy użyciu typów `Value` i `Array` - jeśli jednak nie ma takiej konieczności, zazwyczaj lepszym wyborem będzie `Queue`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a9541-5ed7-4dbf-b2cf-301d054b858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Value, Array\n",
    "\n",
    "def f(n, a):\n",
    "    n.value = 3.1415927\n",
    "    for i in range(len(a)):\n",
    "        a[i] = -a[i]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num = Value('d', 0.0)\n",
    "    arr = Array('i', range(10))\n",
    "\n",
    "    p = Process(target=f, args=(num, arr))\n",
    "    p.start()\n",
    "    p.join()\n",
    "\n",
    "    print(num.value)\n",
    "    print(arr[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166c02c-1753-4bf1-98d9-5c0d6054ade3",
   "metadata": {},
   "source": [
    "### Obiekty Future\n",
    "Wynikiem zasubmitowania funkcji do wykonania na executorze - niezależnie od wybranej implementacji - jest obiekt klasy Future, który enkapsuluje asynchroniczne jej wykonanie. Interfejs tej klasy wygląda mniej więcej tak:\n",
    "```\n",
    "class Future:\n",
    "    def cancel() -> bool:\n",
    "        ...\n",
    "\n",
    "    def cancelled() -> bool:\n",
    "        ...\n",
    "\n",
    "    def running() -> bool:\n",
    "        ...\n",
    "\n",
    "    def done() -> bool:\n",
    "        ...\n",
    "\n",
    "    def result(timeout: int =None) -> Any:\n",
    "        ...\n",
    "\n",
    "    def exception(timeout=None) -> Exception:\n",
    "        ...\n",
    "\n",
    "    def add_done_callback(fn):\n",
    "        ...\n",
    "```\n",
    "Zazwyczaj jednak korzysta się po prostu z funkcji `as_completed` by iterować po kolejnych wynikach kończących sie asynchronicznych obliczeń:\n",
    "```\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(foo, threadNo) for threadNo in range(5)]\n",
    "    for f in concurrent.futures.as_completed(futures):\n",
    "        ...\n",
    "```\n",
    "\n",
    "### freeze_support\n",
    "Drobną dygresją może być to, że aby nasze programy napisane z użyciem modułu `multiprocessing` lub z użyciem `concurrent.futures.ProcessPoolExecutor`a działały bez problemu (zwłaszcza na Windowsie), konieczne jest dodanie kilku *magicznych linijek*:\n",
    "- nasz program powinien mieć klauzulę `if __name__ == \"__main__\"` - jeśli metoda tworzenia procesów to spawn, to każda instancja procesu-dziecka wykonuje na początku _cały_ kod na poziomie modułów. Nie chcemy by kod tworzący nowy proces był wywoływany ponownie dla każdego dziecka rekurencyjnie\n",
    "- ... a w niej na początku `multiprocessing.freeze_support()`\n",
    "Funkcja `freeze_support()` jest niezbędna głównie na systemie Windows oraz w przypadku \"zamrażania\" aplikacji (tworzenia samodzielnych plików wykonywalnych) na macOS, zwłaszcza od Pythona 3.8, gdzie domyślną metodą uruchamiania jest `spawn` a nie `fork`. Pomimo że macOS i Windows używają metody `spawn` w różny sposób, dla obu systemów zaleca się użycie `freeze_support()` przy tworzeniu samodzielnych aplikacji. Jeśli nie \"zamrażamy\" swojego kodu, funkcja `freeze_support()` nie jest wymagana, ale jej dodanie jest dobrą praktyką w celu zapewnienia przyszłej kompatybilności.\n",
    "\n",
    "## Demo: DataLoader\n",
    "\n",
    "## Semafory i zmienne warunkowe\n",
    "Najprostszym typem semafora jest oczywiście mutex, który w Pythonie nazywa się `Lock`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf859c-17be-4b74-8514-0f0a80e69916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Lock\n",
    "\n",
    "def f(l, i):\n",
    "    l.acquire()\n",
    "    try:\n",
    "        print('hello world', i)\n",
    "    finally:\n",
    "        l.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lock = Lock()\n",
    "\n",
    "    for num in range(10):\n",
    "        Process(target=f, args=(lock, num)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e032f-3595-4094-88d9-694604c5a74d",
   "metadata": {},
   "source": [
    "... są również generalniejsze semafory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1f8c1-80c8-4b3e-86a6-64ec8d48d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_psql_conns = 5\n",
    "...\n",
    "with BoundedSemaphore(value=max_psql_conns):\n",
    "    conn = connectdb()\n",
    "    try:\n",
    "        # ... use connection ...\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79d5e4-1f3c-449f-9606-7b664777b636",
   "metadata": {},
   "source": [
    "... i zmienne warunkowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad35372-a796-44c1-8c6d-c48583908cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(cond):\n",
    "    with cond:\n",
    "        while some_queue.isEmpty():\n",
    "            cond.wait()\n",
    "    print('Resource acquired')\n",
    "\n",
    "\n",
    "def producer(cond):\n",
    "    print('Making resource available')\n",
    "    cond.notifyAll()\n",
    "\n",
    "condition = threading.Condition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc7efc2-2d6f-4da8-82bb-20ef1506aec3",
   "metadata": {},
   "source": [
    "### *Zadanie*\n",
    "Poniżej podany jest algorytm \"sita Eratostenesa\". Przy użyciu `concurrent.futures` dla każdego `0 < n <= 1 000 000` sprawdź ile jest liczb pierwszych mniejszych od danego `n`. Załóż na potrzeby zadania, że z jakiegoś powodu musisz wykonywać algorytm dla każdej liczby od nowa.\n",
    "* [ ] napisz `contextmanager`, który zmierzy czas obliczenia wszystkich liczb spełniających warunki zadania\n",
    "* [ ] użyj `concurrent.futures` by rozdysponować pracę między wątki\n",
    "* [ ] użyj `concurrent.futures` by rozdysponować pracę między procesy - jaka jest różnica w szybkości programu?\n",
    "* [ ] zainstaluj `pstree` i zweryfikuj ile procesów lub wątków działa w trakcie wykonania Twojego programu\n",
    "* [ ] jak zmienia się czas wykonania w zależności od ilości wątków/procesów podanych jako parametr `max_workers` executora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958aae7-66b8-46fc-aac6-74fbfd62d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sieve(n):\n",
    "    prime = [True for i in range(n + 1)]\n",
    "    p = 2\n",
    "    while (p * p <= n):\n",
    "        if (prime[p] == True):\n",
    "            for i in range(p * p, n + 1, p):\n",
    "                prime[i] = False\n",
    "        p += 1\n",
    "    result = []\n",
    "    for p in range(2, n + 1):\n",
    "        if prime[p]:\n",
    "            result.append(p)\n",
    "    return len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7568262-c6b2-4214-9c90-f0a55add31e7",
   "metadata": {},
   "source": [
    "### *Zadanie*\n",
    "```\n",
    "git checkout task-visualizer-runner\n",
    "pip-sync\n",
    "```\n",
    "Obecnie moduł `visualizer.py` wykonuje swój kod w głównym wątku aplikacji. W przypadku, gdy lista tickerów do przetworzenia jest duża, może to być wyjątkowo nieefektywne i zajmować wiele czasu. Chcemy użyć pewnej formy zrównoleglenia, tak by teoretycznie móc przetwarzać kilka tickerów na raz. W pakiecie `reporting` w module `visualizer_runner.py` zdefiniowany jest interfejs `VisualizerRunner`a, którego odpowiedzialnością jest uruchamianie istniejącego `Visualizera` w sposób równoległy.\n",
    "- [ ] określ jaka forma zrównoleglenia będzie potencjalnie najlepsza. Zanim przystąpisz do wykonywania kolejnych etapów przedyskutujmy razem nasze podejście\n",
    "- [ ] zaimplementuj podklasę `VisualizerRunner`a, która implementuje go w sposób jednowątkowy\n",
    "- [ ] zaimplementuj podklasę `VisualizerRunner`a, która implementuje go w sposób równoległy zgodnie z wynikiem dyskusji\n",
    "- [ ] uruchom program poleceniem `time python -m stock_trader --start 2008-01-01 --end 2009-01-01 --ticker-list-file ../test_tickers_100.txt --data-source local demo-plot`\n",
    "- [ ] zmień w `src/settings.py` wartość `visualizer_runner_concurrency` na wartość reprezentującą wykonanie równoległe\n",
    "- [ ] ponownie uruchom program\n",
    "- [ ] czy można wysunąć jakieś wnioski na temat działania programu w wersji równoległej względem jednowątkowej?\n",
    "- [ ] jaka inna architektura aplikacji mogłaby pomóc osiągnąć lepsze rezultaty?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b0547-6b5d-4697-b3af-f072de49a68b",
   "metadata": {},
   "source": [
    "## Async/await\n",
    "Oprócz \"tradycyjnych\" form współbieżności i równoległości wykonania opartych o wątki i procesy, Python3 wprowadził wsparcie dla korutyn i programowania asynchronicznego. Korutyny to funkcje, które na czas trwania pewnej wolnej operacji - zazwyczaj IO - mogą zostać zawieszone, a gdy operacja się zakonczy ponownie wznowione. Historycznie podobną funkcjonalność oferowały generatory i pierwsze podejście do tego tematu w Pythonie opierało się na nich, jednak było ono dośc niewygodne i nieintuicyjne. Podobnie jak w C# i Node.js, od Pythona 3.7 istnieje możliwość definiowania korutyn przy użyciu słów kluczowych `async` i `await`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "316bf8df-9d8a-4eeb-b59e-b8aae418f5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object fun2 at 0x107ddca00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun(x): # zwykła funkcja\n",
    "    return x * x\n",
    "\n",
    "fun(5) # zwykłe zawołanie zwykłej funkcji\n",
    "\n",
    "async def fun2(x): # asynchroniczna korutyna\n",
    "    print(\"fun2\")\n",
    "    return x * x\n",
    "\n",
    "await fun2(5) # zawołanie korutyny\n",
    "\n",
    "fun2(6) # zwraca obiekt reprezentujący korutynę która jeszcze nie wykonuje się!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923d2fc-5711-47cc-8ad1-e362166dfb07",
   "metadata": {},
   "source": [
    "Normalnie `await` można użyć tylko wewnątrz korutyny - w przypadku użycia go w zwykłej funkcji dostaniemy błąd:\n",
    "```\n",
    "SyntaxError: 'await' outside async function\n",
    "```\n",
    "Jak w takim razie możemy uruchomić naszą asynchroniczną funkcję w bloku `if __name__ == '__main__'?` Musimy stworzyć event loop - przy użyciu modułu `asyncio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6ee1b-3149-4176-8550-b4cc9b92b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def fun(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    result = await fun(10)\n",
    "    print(f'fun(10)={result}')\n",
    "\n",
    "    result = await fun(5)\n",
    "    print(f'fun(5)={result}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d78608-f402-4019-b383-eb80400d12bc",
   "metadata": {},
   "source": [
    "\n",
    "W Jupyterze nie powyższy kod nie zadziała - Jupyter wewnątrz sam zaimplementowany jest przy użyciu `asyncio`, więc Python nie pozwala na zagnieżdżone stworzenie nowego event loopa - dlatego poprzedni przykład działal :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f21c98-4717-452c-8ca6-3280720df6c7",
   "metadata": {},
   "source": [
    "### Asynchroniczne pętle for, managery kontekstu i generatory\n",
    "Czasami przydatne jest iterowanie po `Iterable`, którego elementy wymagają asynchronicznego wygenerowania - np. przez zapytanie http. Takie asynchroniczne `Iterable` nie może być wołane przez zwykłą pętlę for, która spodziewa się standardowych dunder-metod `__iter__` i `__next__`, które są synchroniczne. Zamiast tego musi ono implementować ich asynchroniczne wersje: `__aiter__` i `__anext__`, które wymagają `await`a. Zwykła, synchroniczna pętla for nie może więc ich wołać (bo `await` jest zakazany poza funkcjami asynchronicznymi). Wtedy używamy zamiast tego asynchronicznej pętli - `async for`. \n",
    "\n",
    "Przydatny przykład jest poniżej - iterujemy po obiektach future zwróconych z `asyncio.as_completed()` po tym jak zakończą się zapytania http z `fetch_stock_data` (w kolejności od najwcześniej zakończonego). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae4fe9-aa0d-48a6-91c9-01271bcc4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "API_KEY = 'TWÓJ_KLUCZ_API'\n",
    "BASE_URL = 'https://www.alphavantage.co/query'\n",
    "\n",
    "async def fetch_stock_data(symbol: str):\n",
    "    params = {\n",
    "        'function': 'TIME_SERIES_DAILY',\n",
    "        'symbol': symbol,\n",
    "        'apikey': API_KEY\n",
    "    }\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(BASE_URL, params=params)\n",
    "        response.raise_for_status()\n",
    "        return symbol, response.json()\n",
    "\n",
    "async def main():\n",
    "    symbols = ['AAPL', 'GOOGL', 'MSFT']\n",
    "\n",
    "    async for future in asyncio.as_completed(fetch_stock_data(symbol) for symbol in symbols):\n",
    "        symbol, data = await future\n",
    "        print(f\"Otrzymano dane dla: {symbol}\")\n",
    "        for date, entry in data.get('Time Series (Daily)', {}).items():\n",
    "            print(f\"Symbol: {symbol}, Data: {date}, Cena zamknięcia: {entry['4. close']}\")\n",
    "\n",
    "# Uruchomienie asynchronicznej funkcji main w pętli zdarzeń\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068fb10-5b9f-44dc-92fa-1230e7f6ebfa",
   "metadata": {},
   "source": [
    "Z analogicznych powodów w Pythonie istnieją asynchroniczne managery kontekstu czy generatory - ich interfejsy również są asynchroniczne więc mogą w swoich implementacjach `await`ować asynchroniczne operacje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb827e-c5c6-4f33-8a2d-c6c952249531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncDatabaseConnection:\n",
    "    async def __aenter__(self):\n",
    "        await self.connect()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        await self.close()\n",
    "\n",
    "    async def connect(self):\n",
    "        # Asynchroniczna logika połączenia z bazą danych\n",
    "\n",
    "    async def close(self):\n",
    "         # Asynchroniczna logika zamknięcia połączenia z bazą danych\n",
    "\n",
    "async with AsyncDatabaseConnection() as conn:\n",
    "    # wykonaj asynchroniczne zapytania\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866272e9-f31b-4028-83ae-b619915f4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "async def stream_http_response(url):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(url, stream=True)  # Włączamy strumieniowanie odpowiedzi\n",
    "\n",
    "        async for chunk in response.aiter_bytes():\n",
    "            yield chunk  # Strumieniowo przetwarzamy kawałki danych\n",
    "\n",
    "async def main():\n",
    "    url = 'https://example.com/large_file'\n",
    "    async for data_chunk in stream_http_response(url):\n",
    "        # Tutaj możemy przetwarzać lub zapisywać kolejne partie danych\n",
    "        print(data_chunk)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8061c-61b2-4360-b40a-193446b1fea0",
   "metadata": {},
   "source": [
    "## Wołanie kodu synchronicznego z asynchronicznego\n",
    "Większość bibliotek w ekosystemie Pythona jest zaimplementowana z użyciem paradygmatu synchronicznego, toteż czasem chcemy wywołać kod synchroniczny z asynchronicznego. Odwrotnie niż w drugą stronę to jest w pełni wspierane i dozwolone, jednak należy uważać na operacje trwające zbyt długo by nie blokować petli zdarzeń - wówczas dobrze skorzystać z dostępnej od Pythona 3.9  funkcji `asyncio.to_thread` i wykonać je po prostu na zewnętrznym wątku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2605ec-2b36-4ed2-bc2a-12a0fecc0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "def long_running_task():\n",
    "    # ... długotrwałe obliczenia ...\n",
    "    return result\n",
    "\n",
    "async def main():\n",
    "    result = await asyncio.to_thread(long_running_task)\n",
    "    # ... reszta kodu ...\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055e21b-9b73-4625-b0b4-7ecc08af7449",
   "metadata": {},
   "source": [
    "W szczególności Python obecnie nie oferuje asynchronicznego API dostepu do plików, toteż operacje ładowania ich do pamięci czy zapisu zazwyczaj powinny korzystać z `to_thread`. Jeśli nie chcemy ich tak opakowywać sami, możemy skorzystać z gotowej biblioteki `aiofile`, która dokładnie to robi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed3429-2048-42ae-8d27-9d343ebb3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "from tempfile import gettempdir\n",
    "\n",
    "from aiofile import async_open\n",
    "\n",
    "tmp_filename = Path(gettempdir()) / \"hello.txt\"\n",
    "\n",
    "async def main():\n",
    "    async with async_open(tmp_filename, 'w+') as afp:\n",
    "        await afp.write(\"Hello \")\n",
    "        await afp.write(\"world\")\n",
    "        afp.seek(0)\n",
    "\n",
    "        print(await afp.read())\n",
    "\n",
    "        await afp.write(\"Hello from\\nasync world\")\n",
    "        print(await afp.readline())\n",
    "        print(await afp.readline())\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
